import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import plotly.express as px
import pydeck as pdk
import pickle
import joblib
import warnings
warnings.filterwarnings('ignore')

# Import ML libraries that might be needed for unpickling
try:
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.pipeline import Pipeline
    from xgboost import XGBClassifier
    import xgboost
except ImportError as e:
    st.warning(f"‚ö†Ô∏è Missing ML library: {e}. Some features may not work.")

# =========================================================
# 0. Setup & Helper Functions
# =========================================================

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Streamlit command)
st.set_page_config(page_title="Traffy Fondue Explorer (Auto Map)", layout="wide")

@st.cache_resource
def load_model():
    """‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Model (.pkl)"""
    try:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        # Use joblib instead of pickle for better compatibility with sklearn/xgboost objects
        return joblib.load('traffy_model_weather.pkl')
    except FileNotFoundError:
        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå 'traffy_model_weather.pkl' - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö script ‡∏ô‡∏µ‡πâ")
        return None
    except (pickle.UnpicklingError, ModuleNotFoundError, AttributeError) as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {type(e).__name__}: {e}")
        st.info("üí° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠: ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:")
        st.code("pip install scikit-learn xgboost pandas numpy joblib", language="bash")
        return None
    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {type(e).__name__}: {e}")
        return None

def preprocess_for_prediction(df, model_pkg):
    """
    [IMPORTANT] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏• XGBoost ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    ‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£ Encode ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Time Features
    """
    from sklearn.preprocessing import LabelEncoder
    
    # ‡∏™‡∏≥‡πÄ‡∏ô‡∏≤ DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    df_processed = df.copy()
    
    # ====== Step 1: Create Time Features from Timestamp ======
    if 'timestamp' in df_processed.columns:
        try:
            df_processed['timestamp'] = pd.to_datetime(df_processed['timestamp'], errors='coerce')
            df_processed['hour'] = df_processed['timestamp'].dt.hour
            df_processed['day_of_week'] = df_processed['timestamp'].dt.dayofweek
            df_processed['month'] = df_processed['timestamp'].dt.month
        except Exception as e:
            st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ï‡∏Å timestamp: {e}")
    
    # ====== Step 2: Create Text Features ======
    if 'comment' in df_processed.columns:
        df_processed['comment_len'] = df_processed['comment'].astype(str).apply(len)
    else:
        df_processed['comment_len'] = 0
    
    # ====== Step 3: Encode Categorical Columns ======
    cols_to_encode = ['district', 'subdistrict', 'type 1']
    org_col = 'organization_1' if 'organization_1' in df_processed.columns else 'organization'
    if org_col in df_processed.columns:
        cols_to_encode.append(org_col)
    
    # ‡πÉ‡∏ä‡πâ Encoders ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
    if model_pkg and 'encoders' in model_pkg:
        encoders_dict = model_pkg['encoders']
    else:
        encoders_dict = {}
    
    for col in cols_to_encode:
        if col in df_processed.columns:
            # ‡πÄ‡∏ï‡∏¥‡∏° Unknown ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
            df_processed[col] = df_processed[col].fillna('Unknown').astype(str)
            
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Encoder ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ô‡∏±‡πâ‡∏ô
            if col in encoders_dict:
                try:
                    # Handle unknown categories (from prediction data)
                    le = encoders_dict[col]
                    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô encoder ‡πÉ‡∏´‡πâ assign 0
                    df_processed[f'{col}_enc'] = df_processed[col].map(
                        lambda x: le.transform([x])[0] if x in le.classes_ else 0
                    )
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ encode {col}: {e}")
                    df_processed[f'{col}_enc'] = 0
            else:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Encoder ‡πÉ‡∏´‡∏°‡πà (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•)
                le = LabelEncoder()
                try:
                    df_processed[f'{col}_enc'] = le.fit_transform(df_processed[col])
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ encode {col}: {e}")
                    df_processed[f'{col}_enc'] = 0
    
    # ====== Step 4: Select Only Required Features ======
    # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Feature ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á (‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Train)
    required_features = [
        'district_enc', 'subdistrict_enc', 'type 1_enc', 
        'organization_1_enc', 'comment_len', 
        'hour', 'day_of_week', 'month'
    ]
    
    # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• feature ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ô‡∏±‡πâ‡∏ô
    if model_pkg and 'features' in model_pkg:
        required_features = model_pkg['features']
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ Feature ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    X = pd.DataFrame()
    for feat in required_features:
        if feat in df_processed.columns:
            X[feat] = df_processed[feat]
        else:
            # ‡∏ñ‡πâ‡∏≤ Feature ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏¥‡∏° 0
            X[feat] = 0
    
    # ====== Step 5: Handle Missing Values ======
    X = X.fillna(0)
    
    # ====== Step 6: Ensure Correct Order & Data Types ======
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)
    
    return X 

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ
model_package = load_model()

st.title("üö¶ Traffy Fondue Analytics (Cluster & Heatmap & Prediction)")

# =========================================================
# 1. Config & Data Loading
# =========================================================

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≠‡πÉ‡∏ô CSV (Key = ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î, Value = ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô CSV)
REQUIRED_COLS_CONFIG = {
    'ticket_id': '‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á (ID)',
    'comment': '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤',
    'organization_1': '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (Organization)',
    'organization_2': '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (Organization)', 
    'organization_3': '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (Organization)',
    'type 1': '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Type)',
    'type 2': '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Type)',
    'type 3': '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Type)',
    'count_reopen': '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥ (Reopen)',
    'star': '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Star)',
    'timestamp': '‡∏ß‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏à‡πâ‡∏á (Timestamp)',
    'province': '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',
    'district': '‡πÄ‡∏Ç‡∏ï/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',
    'subdistrict': '‡πÅ‡∏Ç‡∏ß‡∏á/‡∏ï‡∏≥‡∏ö‡∏•',
    'state': '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ (State)',
    'latitude': '‡∏•‡∏∞‡∏ï‡∏¥‡∏à‡∏π‡∏î (Latitude)',
    'longitude': '‡∏•‡∏≠‡∏á‡∏à‡∏¥‡∏à‡∏π‡∏î (Longitude)',
    'cluster': '‡∏Å‡∏•‡∏∏‡πà‡∏° (Cluster)',
    'coords': 'coords',
    '‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®': '‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®',
    'dist_to_nearest_condo': 'dist_to_nearest_condo_km',
    'avg_price_per_sqm': 'avg_price_per_sqm'
}

@st.cache_data
def load_raw_data():
    return pd.read_csv('scrape.csv')

raw_df = load_raw_data()

@st.cache_data
def cload_cluster_df():
    try:
        return pd.read_csv("clustered_df.csv")
    except:
        return pd.DataFrame()
clusterd_df = cload_cluster_df()

if raw_df.empty:
    st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å (merged_data.csv ‡∏´‡∏£‡∏∑‡∏≠ clean_data2.csv)")
    st.stop()

# =========================================================
# 1.1 Auto Mapping Logic
# =========================================================

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠
rename_dict = {}
found_cols = []

# ‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏≤‡∏° Config
for internal_name, csv_header in REQUIRED_COLS_CONFIG.items():
    if csv_header in raw_df.columns:
        rename_dict[csv_header] = internal_name
        found_cols.append(internal_name)
    elif internal_name in raw_df.columns:
        found_cols.append(internal_name)

# ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
df = raw_df.rename(columns=rename_dict)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå
df = df[found_cols]

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Ñ‡∏£‡∏ö‡πÑ‡∏´‡∏°
missing_critical = []
for crit in ['latitude', 'longitude']:
    if crit not in df.columns:
        missing_critical.append(crit)

if missing_critical:
    st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏û‡∏¥‡∏Å‡∏±‡∏î: {missing_critical} ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏≤‡∏à‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV)")

# --- Process Data ---

# ‡πÅ‡∏õ‡∏•‡∏á Timestamp
if 'timestamp' in df.columns:
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    if pd.api.types.is_datetime64_any_dtype(df['timestamp']):
        if df['timestamp'].dt.tz is not None:
             df['timestamp'] = df['timestamp'].dt.tz_localize(None)

# ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Map)
for col in ['latitude', 'longitude', 'star', 'count_reopen', 'cluster', 'dist_to_nearest_condo', 'avg_price_per_sqm']:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# =========================================================
# 2. Sidebar Filters
# =========================================================
st.sidebar.header("üîç ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1 & 2)")
st.sidebar.markdown("---")

if df.empty:
    st.error("‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
    st.stop()

# Filter Input
n_sample = st.sidebar.slider("1. ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (Sample)", 1, 10000, min(1000, len(df)))

org_options = []
if 'organization_1' in df.columns: 
    org_options = df['organization_1'].dropna().unique()
selected_org = st.sidebar.multiselect("2. ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô", org_options)

type_options = []
if 'type 1' in df.columns:
    type_options = df['type 1'].dropna().unique()
selected_type = st.sidebar.multiselect("3. ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤", type_options)

max_reopen = int(df['count_reopen'].max()) if 'count_reopen' in df.columns and not pd.isna(df['count_reopen'].max()) else 10
reopen_range = st.sidebar.slider("4. ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥", 0, max_reopen, (0, max_reopen))

star_range = st.sidebar.slider("5. ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Star)", 0, 5, (0, 5))

# Weather filter
weather_options = []
if '‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®' in df.columns:
    weather_options = df['‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®'].dropna().unique()
selected_weather = st.sidebar.multiselect("6. ‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®", weather_options)

# Distance to condo filter
if 'dist_to_nearest_condo' in df.columns and df['dist_to_nearest_condo'].notna().sum() > 0:
    min_dist = float(df['dist_to_nearest_condo'].min())
    max_dist = float(df['dist_to_nearest_condo'].max())
    dist_range = st.sidebar.slider("7. ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î‡πÉ‡∏Å‡∏•‡πâ‡∏™‡∏∏‡∏î (km)", min_dist, max_dist, (min_dist, max_dist), step=0.1)
else:
    dist_range = None

# Average price per sqm filter
if 'avg_price_per_sqm' in df.columns and df['avg_price_per_sqm'].notna().sum() > 0:
    min_price = float(df['avg_price_per_sqm'].min())
    max_price = float(df['avg_price_per_sqm'].max())
    price_range = st.sidebar.slider("8. ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠‡∏ï‡∏£.‡∏°. (‡∏ö‡∏≤‡∏ó)", min_price, max_price, (min_price, max_price), step=1000.0)
else:
    price_range = None

if 'timestamp' in df.columns and not df['timestamp'].isna().all():
    min_date = df['timestamp'].min()
    max_date = df['timestamp'].max()
else:
    min_date = datetime.now()
    max_date = datetime.now()

default_date = pd.to_datetime('2022-05-24').date()
date_range = st.sidebar.date_input("9. ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", [default_date, max_date])

selected_prov = st.sidebar.multiselect("10. ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î", df['province'].dropna().unique() if 'province' in df.columns else [])
selected_dist = st.sidebar.multiselect("11. ‡πÄ‡∏Ç‡∏ï/‡∏≠‡∏≥‡πÄ‡∏†‡∏≠", df['district'].dropna().unique() if 'district' in df.columns else [])
selected_sub = st.sidebar.multiselect("12. ‡πÅ‡∏Ç‡∏ß‡∏á/‡∏ï‡∏≥‡∏ö‡∏•", df['subdistrict'].dropna().unique() if 'subdistrict' in df.columns else [])
selected_state = st.sidebar.multiselect("13. ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞", df['state'].dropna().unique() if 'state' in df.columns else [])

# =========================================================
# Sidebar: Prediction Settings
# =========================================================
st.sidebar.markdown("---")
st.sidebar.header("üîÆ ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4)")

uploaded_file = st.sidebar.file_uploader("üìÇ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢", type=['csv'])
run_prediction = st.sidebar.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• (Predict)")

n_pred_sample = st.sidebar.slider("1. ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Pred Sample)", 1, 20000, 2000, help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ô‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á")
pred_dot_size = st.sidebar.slider("2. ‡∏Ç‡∏ô‡∏≤‡∏î‡∏à‡∏∏‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Max Dot Size)", 5, 50, 15, help="‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ß‡∏á‡∏Å‡∏•‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á")

# --- Filtering Logic ---
filtered_df = df.copy()
# Keep original clusterd_df unfiltered for cluster section
clusterd_df_for_cluster = clusterd_df.copy()

# Filter Organization
if selected_org:
    org_conditions = False
    if 'organization_1' in filtered_df.columns:
        org_conditions = org_conditions | filtered_df['organization_1'].isin(selected_org)
    if 'organization_2' in filtered_df.columns:
        org_conditions = org_conditions | filtered_df['organization_2'].isin(selected_org)
    if 'organization_3' in filtered_df.columns:
        org_conditions = org_conditions | filtered_df['organization_3'].isin(selected_org)
    
    if isinstance(org_conditions, pd.Series):
        filtered_df = filtered_df[org_conditions]

# Filter Type (applies to 2.1, 2.2, maps only)
if selected_type:
    type_conditions = False
    if 'type 1' in filtered_df.columns:
        type_conditions = type_conditions | filtered_df['type 1'].isin(selected_type)
    if 'type 2' in filtered_df.columns:
        type_conditions = type_conditions | filtered_df['type 2'].isin(selected_type)
    if 'type 3' in filtered_df.columns:
        type_conditions = type_conditions | filtered_df['type 3'].isin(selected_type)
        
    if isinstance(type_conditions, pd.Series):
        filtered_df = filtered_df[type_conditions]

# Filter Numeric/Date
if 'count_reopen' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['count_reopen'].between(reopen_range[0], reopen_range[1])]

if 'star' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['star'].between(star_range[0], star_range[1])]

# Filter Weather (only for 2.1, 2.2 - NOT cluster)
if selected_weather and '‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®'].isin(selected_weather)]

# Filter Distance to Condo (only for 2.1, 2.2 - NOT cluster)
if dist_range is not None and 'dist_to_nearest_condo' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['dist_to_nearest_condo'].between(dist_range[0], dist_range[1])]

# Filter Average Price per sqm (only for 2.1, 2.2 - NOT cluster)
if price_range is not None and 'avg_price_per_sqm' in filtered_df.columns:
    filtered_df = filtered_df[filtered_df['avg_price_per_sqm'].between(price_range[0], price_range[1])]

if 'timestamp' in filtered_df.columns and isinstance(date_range, tuple) and len(date_range) == 2:
    start_date = pd.to_datetime(date_range[0])
    end_date = pd.to_datetime(date_range[1]) + timedelta(days=1) - timedelta(seconds=1)
    filtered_df = filtered_df[(filtered_df['timestamp'] >= start_date) & (filtered_df['timestamp'] <= end_date)]

# Filter Location/State (applies to both)
if selected_prov: 
    filtered_df = filtered_df[filtered_df['province'].isin(selected_prov)]
    if 'province' in clusterd_df_for_cluster.columns:
        clusterd_df_for_cluster = clusterd_df_for_cluster[clusterd_df_for_cluster['province'].isin(selected_prov)]

if selected_dist: 
    filtered_df = filtered_df[filtered_df['district'].isin(selected_dist)]
    if 'district' in clusterd_df_for_cluster.columns:
        clusterd_df_for_cluster = clusterd_df_for_cluster[clusterd_df_for_cluster['district'].isin(selected_dist)]

if selected_sub: 
    filtered_df = filtered_df[filtered_df['subdistrict'].isin(selected_sub)]
    if 'subdistrict' in clusterd_df_for_cluster.columns:
        clusterd_df_for_cluster = clusterd_df_for_cluster[clusterd_df_for_cluster['subdistrict'].isin(selected_sub)]

if selected_state: 
    filtered_df = filtered_df[filtered_df['state'].isin(selected_state)]
    if 'state' in clusterd_df_for_cluster.columns:
        clusterd_df_for_cluster = clusterd_df_for_cluster[clusterd_df_for_cluster['state'].isin(selected_state)]

# ‡πÅ‡∏¢‡∏Å Dataframe
# analysis_df = ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü 2.1 ‡πÅ‡∏•‡∏∞ 2.2 (‡∏£‡∏±‡∏ö filter ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
analysis_df = filtered_df.copy()
# display_df = ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢ sample)
display_df = filtered_df.head(n_sample)
# Use clusterd_df_for_cluster for cluster section (only location filters)
clusterd_df = clusterd_df_for_cluster.copy()
clusterd_df_display = clusterd_df_for_cluster.head(n_sample)

st.markdown(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö (Filter):** {len(analysis_df):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | **‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•:** {len(display_df):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
st.markdown("---")

# =========================================================
# 3. Visualization (Maps)
# =========================================================

st.header("1. ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏≤‡πÅ‡∏ô‡πà‡∏ô (Map Visualization)")

# ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
if 'latitude' in display_df.columns and 'longitude' in display_df.columns:
    map_data = display_df.dropna(subset=['latitude', 'longitude'])
    map_data = map_data[(map_data['latitude'] != 0) & (map_data['longitude'] != 0)]
else:
    map_data = pd.DataFrame()

if not map_data.empty:
    mid_lat = map_data['latitude'].mean()
    mid_lon = map_data['longitude'].mean()

    view_state = pdk.ViewState(
        latitude=mid_lat,
        longitude=mid_lon,
        zoom=10,
        pitch=0,
    )

    tab_scatter, tab_heat, tab_cluster = st.tabs(["üìç Scatter Plot (‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î)", "üî• Heatmap (‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô)", "Cluster (‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°)"])

    # ---------------- TAB 1: SCATTER ----------------
    with tab_scatter:
        st.caption("‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏£‡∏≤‡∏¢‡∏à‡∏∏‡∏î")
        
        map_data['color'] = [[255, 0, 0, 180]] * len(map_data)

        scatterplot_layer = pdk.Layer(
            "ScatterplotLayer",
            data=map_data,
            get_position='[longitude, latitude]',
            get_fill_color='color',
            get_radius=50,
            pickable=True,
            opacity=0.8,
            stroked=True,
            filled=True,
            radius_min_pixels=3,
            radius_max_pixels=10,
        )

        tooltip_fields = []
        if 'ticket_id' in map_data.columns: tooltip_fields.append("<b>ID:</b> {ticket_id}")
        if 'type 1' in map_data.columns: tooltip_fields.append("<b>Type:</b> {type 1}")
        if 'type 2' in map_data.columns: tooltip_fields.append("<b>Type:</b> {type 2}")
        if 'type 3' in map_data.columns: tooltip_fields.append("<b>Type:</b> {type 3}")
        if 'cluster' in map_data.columns: tooltip_fields.append("<b>Cluster:</b> {cluster}")
        
        tooltip_html = {
            "html": "<br/>".join(tooltip_fields) if tooltip_fields else "No Info",
            "style": {"backgroundColor": "steelblue", "color": "white"}
        }

        st.pydeck_chart(pdk.Deck(
            layers=[scatterplot_layer], 
            initial_view_state=view_state,
            tooltip=tooltip_html
        ))

    # ---------------- TAB 2: HEATMAP ----------------
    with tab_heat:
        st.caption("‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏≤‡πÅ‡∏ô‡πà‡∏ô (Heatmap)")
        
        heatmap_layer = pdk.Layer(
            "HeatmapLayer",
            data=map_data,
            get_position='[longitude, latitude]',
            get_fill_color='color',
            opacity=0.8,
            aggregation_name="SUM",
            radiusPixels=40,    
            intensity=1,
            threshold=0.05      
        )

        st.pydeck_chart(pdk.Deck(
            layers=[heatmap_layer], 
            initial_view_state=view_state
        ))

    # ---------------- TAB 3: CLUSTER ----------------
    with tab_cluster:
        def get_color(cluster_id):
            if cluster_id == 1: return [255, 0, 0, 200]    
            elif cluster_id == 2: return [0, 255, 0, 200]    
            elif cluster_id == 3: return [0, 0, 255, 200]    
            else: return [165, 3, 252, 200] 

        if not clusterd_df_display.empty and 'cluster' in clusterd_df_display.columns:
            clusterd_df_display['color'] = clusterd_df_display['cluster'].apply(get_color)

            view_state = pdk.ViewState(
                latitude=clusterd_df_display['latitude'].mean(),
                longitude=clusterd_df_display['longitude'].mean(),
                zoom=11,
                pitch=0
            )

            scatterplot_layer = pdk.Layer(
                "ScatterplotLayer",
                data=clusterd_df_display,
                get_position='[longitude, latitude]',
                get_fill_color='color',      
                get_radius=200,              
                radius_min_pixels=5,         
                radius_max_pixels=50,
                pickable=True,               
                opacity=0.8,
                stroked=True,
                filled=True
            )

            tooltip = {
                "html": "<b>‡πÄ‡∏Ç‡∏ï:</b> {district} <br/>"
                        "<b>Cluster:</b> {cluster} <br/>"
                        "<b>‡∏õ‡∏±‡∏ç‡∏´‡∏≤:</b> {first_type} <br/>"
                        "<b>‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞:</b> {state} <br/>"
                        "<b>Reopen:</b> {count_reopen}",
                "style": {
                    "backgroundColor": "steelblue",
                    "color": "white"
                }
            }

            st.caption("‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏° (Cluster)")
            st.pydeck_chart(pdk.Deck(
                initial_view_state=view_state,
                layers=[scatterplot_layer],
                tooltip=tooltip
            ))
        else:
            st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Cluster ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ")

else:
    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏¥‡∏Å‡∏±‡∏î (Latitude/Longitude) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å")
    
st.markdown("---")

# =========================================================
# 2. Statistics & Distribution (Updated for Sliders)
# =========================================================

st.header("2. ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (Distribution Analysis)")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2.1: ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏Å‡∏±‡∏ö Slider (Date, Star, Reopen) ---
st.subheader("2.1 ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á (Slider Filters)")
st.subheader(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ticket ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î{filtered_df.shape[0]}")
st.caption("‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà, ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")

col_slide1, col_slide2 = st.columns(2)

# 1. ‡∏Å‡∏£‡∏≤‡∏ü Time Series (‡∏à‡∏≤‡∏Å Date Range Slider)
with col_slide1:
    if 'timestamp' in analysis_df.columns:
        # Create time series data for line chart
        time_series = analysis_df.groupby(analysis_df['timestamp'].dt.date).size().reset_index(name='count')
        time_series.columns = ['timestamp', 'count']
        
        fig_time = px.line(
            time_series, 
            x='timestamp', 
            y='count',
            title="üìà ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ (Time Distribution)",
            markers=True,
            color_discrete_sequence=['#00CC96']
        )
        fig_time.update_layout(xaxis_title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", yaxis_title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á", hovermode='x unified')
        st.plotly_chart(fig_time, use_container_width=True)
    else:
        st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ (timestamp)")

# 2. ‡∏Å‡∏£‡∏≤‡∏ü Star (‡∏à‡∏≤‡∏Å Star Slider)
with col_slide2:
    if 'star' in analysis_df.columns:
        fig_star = px.histogram(
            analysis_df, 
            x='star', 
            title="‚≠ê ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Star Distribution)",
            nbins=11, 
            range_x=[-0.5, 5.5],
            color_discrete_sequence=['#FFD700']
        )
        fig_star.update_layout(xaxis_title="‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Star)", yaxis_title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á", bargap=0.2)
        st.plotly_chart(fig_star, use_container_width=True)
    else:
        st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (star)")

# 3. ‡∏Å‡∏£‡∏≤‡∏ü Reopen (‡∏à‡∏≤‡∏Å Reopen Slider)
if 'count_reopen' in analysis_df.columns:
    fig_reopen = px.histogram(
        analysis_df, 
        x='count_reopen', 
        title="üîÑ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥ (Reopen Count Distribution)",
        color_discrete_sequence=['#EF553B']
    )
    fig_reopen.update_layout(xaxis_title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥", yaxis_title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
    st.plotly_chart(fig_reopen, use_container_width=True)

# 3.1 ‡∏Å‡∏£‡∏≤‡∏ü Top 5 Types with Most Reopen Count
if 'count_reopen' in analysis_df.columns and 'type 1' in analysis_df.columns:
    # Get top 5 types by total reopen count
    type_reopen = analysis_df.groupby('type 1')['count_reopen'].sum().nlargest(5).reset_index()
    type_reopen.columns = ['type', 'total_reopen']
    
    fig_top5_reopen = px.bar(
        type_reopen, 
        x='type', 
        y='total_reopen', 
        title="üèÜ Top 5 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Top 5 Problem Types by Reopen Count)",
        color='total_reopen',
        color_continuous_scale='Reds',
        labels={'type': '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Problem Type)', 'total_reopen': '‡∏£‡∏ß‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥'}
    )
    fig_top5_reopen.update_xaxes(tickangle=-45)
    fig_top5_reopen.update_layout(showlegend=False)
    st.plotly_chart(fig_top5_reopen, use_container_width=True)

st.markdown("---")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2.2: ‡∏Å‡∏£‡∏≤‡∏ü‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ---
st.subheader("2.2 ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Problem Type Distribution)")

if 'type 1' in analysis_df.columns:
    if analysis_df['type 1'].notna().sum() > 0:
        problem_type_counts = analysis_df['type 1'].value_counts().reset_index()
        problem_type_counts.columns = ['type', 'count']
        fig_type = px.bar(
            problem_type_counts, 
            x='type', 
            y='count', 
            title="‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Problem Type)",
            color='count',
            color_continuous_scale='Viridis',
            labels={'type': '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤', 'count': '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á'}
        )
        fig_type.update_xaxes(tickangle=-45)
        fig_type.update_layout(showlegend=False)
        st.plotly_chart(fig_type, use_container_width=True)
    else:
        st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤")

st.markdown("---")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2.3: ‡∏Å‡∏£‡∏≤‡∏ü‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏≠‡∏∑‡πà‡∏ô‡πÜ ---
st.subheader("2.3 ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")

col1, col2 = st.columns(2)
other_cols = ['subdistrict', 'district', 'province', 'state', '‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®'] 

for i, col_name in enumerate(other_cols):
    with (col1 if i % 2 == 0 else col2):
        if col_name in analysis_df.columns:
            if analysis_df[col_name].notna().sum() > 0:
                top_values = analysis_df[col_name].value_counts().nlargest(15).index
                filtered_chart_df = analysis_df[analysis_df[col_name].isin(top_values)]
                
                fig = px.histogram(
                    filtered_chart_df, 
                    x=col_name, 
                    title=f"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á {col_name} (Top 15)",
                    color_discrete_sequence=['#636EFA']
                )
                fig.update_xaxes(tickangle=-45)
                st.plotly_chart(fig, use_container_width=True)

# Additional numeric distributions
st.subheader("2.4 ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç")
col3, col4 = st.columns(2)

with col3:
    if 'dist_to_nearest_condo' in analysis_df.columns and analysis_df['dist_to_nearest_condo'].notna().sum() > 0:
        fig_dist = px.histogram(
            analysis_df, 
            x='dist_to_nearest_condo', 
            title="üìç ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î‡πÉ‡∏Å‡∏•‡πâ‡∏™‡∏∏‡∏î (km)",
            color_discrete_sequence=['#AB63FA'],
            nbins=30
        )
        fig_dist.update_layout(xaxis_title="‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á (km)", yaxis_title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
        st.plotly_chart(fig_dist, use_container_width=True)

with col4:
    if 'avg_price_per_sqm' in analysis_df.columns and analysis_df['avg_price_per_sqm'].notna().sum() > 0:
        fig_price = px.histogram(
            analysis_df, 
            x='avg_price_per_sqm', 
            title="üí∞ ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏°‡∏ï‡∏£ (‡∏ö‡∏≤‡∏ó)",
            color_discrete_sequence=['#FFA15A'],
            nbins=30
        )
        fig_price.update_layout(xaxis_title="‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡∏ö‡∏≤‡∏ó/‡∏ï‡∏£.‡∏°.)", yaxis_title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á")
        st.plotly_chart(fig_price, use_container_width=True)

st.markdown("---")



# =========================================================
# 3. Cluster Analysis
# =========================================================
st.header("3. ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Cluster Analysis)")

if 'cluster' in clusterd_df.columns:
    cluster_data = clusterd_df.copy()
    cluster_data['cluster'] = cluster_data['cluster'].astype(str)
    
    unique_clusters = sorted([c for c in cluster_data['cluster'].unique() if c != 'nan' and c != 'None'], key=lambda x: int(float(x)) if x.replace('.','',1).isdigit() else x)

    # 3.1 ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Cluster
    st.subheader("3.1 ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Cluster")
    total_counts = cluster_data.groupby('cluster').size().reset_index(name='count')
    total_counts = total_counts.sort_values('cluster', key=lambda col: col.map(lambda x: int(float(x)) if x.replace('.','',1).isdigit() else x))
    
    fig_total = px.bar(
        total_counts, x='cluster', y='count', color='cluster',
        title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° Cluster", labels={'cluster': 'Cluster', 'count': '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á'}, text_auto=True
    )
    st.plotly_chart(fig_total, use_container_width=True)

    # 3.2 ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô
    st.subheader("3.2 ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô (‡∏Ñ‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô % ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Cluster)")
    state_cluster_counts = cluster_data.groupby(['state', 'cluster']).size().reset_index(name='count')
    total_cluster_counts = state_cluster_counts.groupby('cluster')['count'].sum().reset_index(name='total_cluster_count')
    state_cluster_counts = pd.merge(state_cluster_counts, total_cluster_counts, on='cluster')
    state_cluster_counts['percentage'] = (state_cluster_counts['count'] / state_cluster_counts['total_cluster_count']) * 100
    
    fig_state_cluster = px.bar(
        state_cluster_counts, x="state", y="percentage", color="cluster",
        title="‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô (% ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Cluster ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á)",
        labels={"state": "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞", "percentage": "‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô (%)", "cluster": "Cluster"},
        barmode='group', text_auto='.1f'
    )
    fig_state_cluster.update_layout(yaxis_ticksuffix="%")
    st.plotly_chart(fig_state_cluster, use_container_width=True)

    # 3.3 Average Reopen
    st.subheader("3.3 ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥ (Average Reopen) ‡∏£‡∏≤‡∏¢ Cluster")
    if 'count_reopen' in cluster_data.columns:
        avg_reopen = cluster_data.groupby('cluster')['count_reopen'].mean().reset_index()
        avg_reopen = avg_reopen.sort_values('cluster', key=lambda col: col.map(lambda x: int(float(x)) if x.replace('.','',1).isdigit() else x))

        fig_reopen_bar = px.bar(
            avg_reopen, x='cluster', y='count_reopen', color='cluster',
            title="‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥ (Reopen) ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Cluster",
            labels={'cluster': 'Cluster', 'count_reopen': '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢'}, text_auto='.2f'
        )
        st.plotly_chart(fig_reopen_bar, use_container_width=True)

    # 3.4 Top 3
    st.subheader("3.4 ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Top 3 ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÅ‡∏•‡∏∞ ‡πÄ‡∏Ç‡∏ï ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Cluster")
    if len(unique_clusters) > 0:
        st.markdown("##### üìå Top 3 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (First Type)")
        cols_type = st.columns(len(unique_clusters)) 
        for i, cluster_id in enumerate(unique_clusters):
            with cols_type[i]:
                subset = cluster_data[cluster_data['cluster'] == cluster_id]
                col_type_name = 'type 1' if 'type 1' in subset.columns else 'first_type'
                
                if col_type_name in subset.columns:
                    top_types = subset[col_type_name].value_counts().nlargest(3).reset_index()
                    top_types.columns = ['type', 'count']
                    fig_type = px.bar(top_types, x='type', y='count', title=f"Cluster {cluster_id}", text_auto=True, color_discrete_sequence=['#FF7F0E'])
                    fig_type.update_layout(xaxis_title=None, yaxis_title=None, margin=dict(l=10, r=10, t=40, b=10))
                    st.plotly_chart(fig_type, use_container_width=True)

        st.markdown("##### üèôÔ∏è Top 3 ‡πÄ‡∏Ç‡∏ï (District)")
        cols_dist = st.columns(len(unique_clusters)) 
        for i, cluster_id in enumerate(unique_clusters):
            with cols_dist[i]:
                subset = cluster_data[cluster_data['cluster'] == cluster_id]
                if 'district' in subset.columns:
                    top_dists = subset['district'].value_counts().nlargest(3).reset_index()
                    top_dists.columns = ['district', 'count']
                    fig_dist = px.bar(top_dists, x='district', y='count', title=f"Cluster {cluster_id}", text_auto=True, color_discrete_sequence=['#2CA02C'])
                    fig_dist.update_layout(xaxis_title=None, yaxis_title=None, margin=dict(l=10, r=10, t=40, b=10))
                    st.plotly_chart(fig_dist, use_container_width=True)

    # 3.5 Top 3 Problem Specific
    st.subheader("3.5 ‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (First Type) ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Cluster")
    col_problem_name = 'first_type' if 'first_type' in cluster_data.columns else 'type 1'
    if len(unique_clusters) > 0 and col_problem_name in cluster_data.columns:
        cols = st.columns(len(unique_clusters))
        for i, cluster_id in enumerate(unique_clusters):
            with cols[i]:
                subset = cluster_data[cluster_data['cluster'] == cluster_id]
                top_problems = subset[col_problem_name].value_counts().nlargest(3).reset_index()
                top_problems.columns = ['first_type', 'count']
                fig_prob = px.bar(top_problems, x='first_type', y='count', title=f"<b>Cluster {cluster_id}</b>", text_auto=True, color_discrete_sequence=['#FF5733'], height=350)
                fig_prob.update_layout(xaxis_title=None, yaxis_title=None, margin=dict(l=10, r=10, t=40, b=10), showlegend=False)
                st.plotly_chart(fig_prob, use_container_width=True)

else:
    st.info("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'cluster' ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏î‡πâ")

st.markdown("---")

# =========================================================
# 4. PREDICTION LOGIC
# =========================================================
st.header("4. ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏ã‡πâ‡∏≥ (Reopen Risk Prediction)")
st.caption("‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Upload ‡πÑ‡∏ü‡∏•‡πå CSV + ‡πÅ‡∏õ‡∏•‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")

final_pred_df = pd.DataFrame()

if uploaded_file is not None and run_prediction:
    if model_package is None:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• 'traffy_model_weather.pkl' ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö")
    else:
        with st.spinner("‚òÅÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡πÅ‡∏õ‡∏•‡∏á‡∏û‡∏¥‡∏Å‡∏±‡∏î + ‡∏î‡∏∂‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏® + ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•)..."):
            try:
                raw_up = pd.read_csv(uploaded_file)
                up_rename = {}
                for k, v in REQUIRED_COLS_CONFIG.items():
                    if v in raw_up.columns: up_rename[v] = k
                
                df_up = raw_up.rename(columns=up_rename)

                if 'coords' in df_up.columns and ('latitude' not in df_up.columns or 'longitude' not in df_up.columns):
                    try:
                        coords_split = df_up['coords'].astype(str).str.split(',', expand=True)
                        if coords_split.shape[1] >= 2:
                            df_up['longitude'] = pd.to_numeric(coords_split[0], errors='coerce')
                            df_up['latitude'] = pd.to_numeric(coords_split[1], errors='coerce')
                            st.success(f"‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'coords' ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(df_up)} ‡πÅ‡∏ñ‡∏ß)")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á coords ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

                if 'latitude' not in df_up.columns:
                    st.warning("‚ö†Ô∏è ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏û‡∏¥‡∏Å‡∏±‡∏î (latitude/longitude ‡∏´‡∏£‡∏∑‡∏≠ coords) ‡∏à‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà")
                    
                X_input = preprocess_for_prediction(df_up, model_package) 
                
                if model_package and 'model' in model_package:
                    model = model_package['model']
                    threshold_calc = 0.723 
                    try:
                        probs = model.predict_proba(X_input)[:, 1]
                        df_up['reopen_probability'] = probs
                        df_up['risk_level'] = df_up['reopen_probability'].apply(lambda x: 'High' if x > threshold_calc else 'Low')
                        final_pred_df = df_up
                    except Exception as e:
                        st.error(f"Prediction Error: {e} (‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô preprocess_for_prediction)")
                else:
                    st.error("Model format invalid")
                
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {e}")

# Result Display
if not final_pred_df.empty:
    if 'latitude' in final_pred_df.columns and 'longitude' in final_pred_df.columns:
        pred_display = final_pred_df.head(n_pred_sample)
        map_display = pred_display.dropna(subset=['latitude', 'longitude'])
        
        if 'reopen_probability' in map_display.columns:
            map_display['adjusted_prob'] = map_display['reopen_probability'] - 0.2
            threshold_display = 0.569
            map_display['risk_color'] = map_display['adjusted_prob'].apply(
                lambda x: '‡∏™‡∏π‡∏á (High Risk)' if x > threshold_display else '‡∏ï‡πà‡∏≥ (Low Risk)'
            )
            
            st.info(f"üìç ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà: {len(map_display):,} ‡∏à‡∏∏‡∏î | Threshold: {threshold_display:.4f}")
            
            if not map_display.empty:
                fig_risk = px.scatter_mapbox(
                    map_display,
                    lat="latitude", lon="longitude",
                    color="risk_color", size="reopen_probability",
                    hover_name="ticket_id" if "ticket_id" in map_display.columns else None,
                    hover_data={
                        "risk_level": True, "reopen_probability": ":.2f", "risk_color": False,
                        "district": True if 'district' in map_display.columns else False,
                        "type": True if 'type' in map_display.columns else False
                    },
                    color_discrete_map={'‡∏™‡∏π‡∏á (High Risk)': '#FF4444', '‡∏ï‡πà‡∏≥ (Low Risk)': '#44FF44'},
                    size_max=pred_dot_size, zoom=10, height=600,
                    title=f"‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á (‡πÅ‡∏î‡∏á = ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á | ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß = ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡πà‡∏≥)"
                )
                fig_risk.update_layout(
                    mapbox_style="carto-positron",
                    mapbox_center={"lat": map_display['latitude'].mean(), "lon": map_display['longitude'].mean()},
                    margin={"r":0,"t":40,"l":0,"b":0}
                )
                st.plotly_chart(fig_risk, use_container_width=True)
        else:
            st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå reopen_probability ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
    else:
        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Latitude/Longitude")

    st.subheader("üìã ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
    csv = final_pred_df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label="üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (CSV)",
        data=csv,
        file_name="prediction_result.csv",
        mime="text/csv"
    )

else:
    st.info("üëà ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡πÅ‡∏ñ‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° 'üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• (Predict)'")
    st.markdown(
        """
        <div style='background-color: #f8f9fa; border: 2px dashed #ccc; border-radius: 10px; padding: 60px; text-align: center; color: #888;'>
            <h2 style='color: #ccc;'>üîÆ</h2>
            <h3>‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢</h3>
            <p>‡∏£‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Upload...</p>
        </div>
        """,
        unsafe_allow_html=True
    )