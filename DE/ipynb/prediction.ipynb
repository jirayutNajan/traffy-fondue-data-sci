{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiFN9jlTyQBY",
        "outputId": "57093c5f-8520-4394-938f-9bfcd60caf19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -> Train Size: 209068 | Test Size: 52268\n",
            "\n",
            "2. Applying Undersampling...\n",
            "\n",
            "3. Training XGBoost Model...\n",
            "\n",
            "4. Optimizing Decision Threshold...\n",
            " -> Optimal Threshold: 0.5583\n",
            "\n",
            "========================================\n",
            "       FINAL MODEL PERFORMANCE\n",
            "========================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.79      0.86     48679\n",
            "           1       0.11      0.34      0.17      3589\n",
            "\n",
            "    accuracy                           0.76     52268\n",
            "   macro avg       0.53      0.57      0.51     52268\n",
            "weighted avg       0.89      0.76      0.81     52268\n",
            "\n",
            "\n",
            "Saving Model to 'traffy_model.pkl'...\n",
            "✅ Saved successfully! File: 'traffy_model.pkl'\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# FAST AI MODULE: XGBoost (No Condo Data) + Export .pkl\n",
        "# ==============================================================================\n",
        "# pip install imbalanced-learn xgboost joblib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from xgboost import XGBClassifier\n",
        "import joblib  # สำหรับ Save Model\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Data Preparation & Feature Engineering\n",
        "# ---------------------------------------------------------\n",
        "# เปลี่ยน Path ไฟล์ของคุณที่นี่\n",
        "file_path = '/opt/airflow/data/clean_data2.csv'\n",
        "\n",
        "try:\n",
        "    df_ml = pd.read_csv(file_path)\n",
        "except FileNotFoundError:\n",
        "    print(\"⚠️ หาไฟล์ไม่เจอ ใช้ Dummy Data แทนเพื่อทดสอบ Code\")\n",
        "    # สร้าง Data จำลอง (ตัดส่วน Condo ออกแล้ว)\n",
        "    df_ml = pd.DataFrame({\n",
        "        'count_reopen': np.random.choice([0, 1, 2], 1000, p=[0.7, 0.2, 0.1]),\n",
        "        'timestamp': pd.date_range(start='1/1/2023', periods=1000),\n",
        "        'comment': ['ทดสอบรายละเอียดปัญหา']*1000,\n",
        "        'district': np.random.choice(['เขต A', 'เขต B', 'เขต C'], 1000),\n",
        "        'subdistrict': np.random.choice(['แขวง 1', 'แขวง 2'], 1000),\n",
        "        'type 1': np.random.choice(['ถนน', 'ทางเท้า', 'ความสะอาด'], 1000),\n",
        "        'organization_1': np.random.choice(['สนง.เขต', 'สำนักการโยธา'], 1000),\n",
        "    })\n",
        "\n",
        "# Target: 1 = Reopen, 0 = Done\n",
        "df_ml['is_reopen'] = (df_ml['count_reopen'] > 0).astype(int)\n",
        "\n",
        "# Time Features\n",
        "df_ml['timestamp'] = pd.to_datetime(df_ml['timestamp'], errors='coerce')\n",
        "df_ml['hour'] = df_ml['timestamp'].dt.hour\n",
        "df_ml['day_of_week'] = df_ml['timestamp'].dt.dayofweek\n",
        "df_ml['month'] = df_ml['timestamp'].dt.month\n",
        "\n",
        "# Text Feature: ความยาวคำร้องเรียน\n",
        "df_ml['comment_len'] = df_ml['comment'].astype(str).apply(len)\n",
        "\n",
        "# --- [ENCODING] ---\n",
        "# เก็บ Encoder ใส่ Dictionary เพื่อ Save ไปใช้ทีหลัง\n",
        "encoders_dict = {}\n",
        "\n",
        "cols_to_encode = ['district', 'subdistrict', 'type 1']\n",
        "# เช็คชื่อคอลัมน์ Organization\n",
        "org_col = 'organization_1' if 'organization_1' in df_ml.columns else 'organization'\n",
        "if org_col in df_ml.columns:\n",
        "    cols_to_encode.append(org_col)\n",
        "\n",
        "for col in cols_to_encode:\n",
        "    if col in df_ml.columns:\n",
        "        # Fill NA และแปลงเป็น String\n",
        "        df_ml[col] = df_ml[col].fillna('Unknown').astype(str)\n",
        "\n",
        "        # สร้าง Encoder\n",
        "        le = LabelEncoder()\n",
        "        df_ml[f'{col}_enc'] = le.fit_transform(df_ml[col])\n",
        "\n",
        "        # เก็บ Encoder\n",
        "        encoders_dict[col] = le\n",
        "\n",
        "# Feature Selection (เอา Condo ออกแล้ว)\n",
        "features = [\n",
        "    # Location & Type\n",
        "    f'{cols_to_encode[0]}_enc',                     # District\n",
        "    f'{cols_to_encode[1]}_enc',                     # Subdistrict\n",
        "    f'{cols_to_encode[2]}_enc',                     # Issue Type\n",
        "\n",
        "    # Organization\n",
        "    f'{org_col}_enc',\n",
        "\n",
        "    # Complexity\n",
        "    'comment_len',\n",
        "\n",
        "    # Time\n",
        "    'hour', 'day_of_week', 'month'\n",
        "]\n",
        "\n",
        "# กรองเฉพาะ Feature ที่มีอยู่จริงใน DataFrame (กัน Error)\n",
        "valid_features = [f for f in features if f in df_ml.columns]\n",
        "\n",
        "# Clean Missing Data\n",
        "df_ready = df_ml.dropna(subset=valid_features)\n",
        "X = df_ready[valid_features]\n",
        "y = df_ready['is_reopen']\n",
        "\n",
        "# Split Train/Test (80:20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\" -> Train Size: {X_train.shape[0]} | Test Size: {X_test.shape[0]}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Handling Imbalance with Undersampling\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n2. Applying Undersampling...\")\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Training XGBoost Model\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n3. Training XGBoost Model...\")\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "xgb_model.fit(X_train_rus, y_train_rus)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Threshold Optimization\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n4. Optimizing Decision Threshold...\")\n",
        "y_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(f\" -> Optimal Threshold: {best_threshold:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Final Evaluation & SAVE MODEL\n",
        "# ---------------------------------------------------------\n",
        "y_pred_final = (y_proba >= best_threshold).astype(int)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"       FINAL MODEL PERFORMANCE\")\n",
        "print(\"=\"*40)\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "\n",
        "# --- [SAVE MODEL] ---\n",
        "print(\"\\nSaving Model to 'traffy_model.pkl'...\")\n",
        "\n",
        "model_package = {\n",
        "    'model': xgb_model,\n",
        "    'encoders': encoders_dict,\n",
        "    'threshold': best_threshold,\n",
        "    'features': valid_features,\n",
        "    'org_col_name': org_col\n",
        "}\n",
        "\n",
        "joblib.dump(model_package, 'traffy_model.pkl')\n",
        "print(\"✅ Saved successfully! File: 'traffy_model.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51849987"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
